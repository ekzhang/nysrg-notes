- Copy-and-patch JIT — [[June 29th, 2025]]
    - https://fredrikbk.com/publications/copy-and-patch.pdf
    - "different points on the startup delay-execution performance Pareto frontier."
        - The paper actually draws out this frontier, averaged over Wasm benchmarks and TPC-H on databases! It's pretty interesting to compare LLVM to Wasm, V8, and this approach.
    - The copy-and-patch paper makes a distinction between "full compilers" as in a database query language compiler (all of which currently rely on LLVM), and "bytecode assemblers" that work on a linearized bytecode. They make a full compiler, on a C-like language that can be generated by embedded functions in C++, turned into an AST, and then compiled.
    - "At a high level, copy-and-patch works by having a pre-built library of composable and parametrizable binary code snippets that we call binary stencils."
        - Wasm = 1666 stencils (35 kB), high-level C compiler = 98,831 stencils (17.5 MB).
        - So an average stencil is in the range of 10-500 bytes.
    - They take stencils and then compose them into a full representation by patching them, including some linearization passes. Each stencil is generated ahead-of-time, and then actually combining them has very little performance impact.
    - Can be extended with other optimizations (unrolling, common subexpressions, SIMD, …).
    - MetaVar = their system for producing binary stencils, from clean C++ code and LLVM.
        - Multiple stencil variants for each AST node. For example, addition with a constant / literal, versus addition with a register or stack value.
        - Differs from template JITs in that there are more stencils per node 
        - Stencils are not generated by hand, there are too many. Each is a binary code fragment with holes for literals, jump addresses, and stack offsets. They represent either an AST node, or a commonly-used AST subtree / bytecode sequence ("supernode").
        - They use continuation-passing style / tail recursion, no returns. Uses Clang TCO to turn the calls into jumps and GHC calling convention for full caller-saved registers.
            - Note: GHC calling convention is because Haskell needs lots of continuations. Its IR is based on System F.
        - Repurposes calling convention into register allocation: each function argument is implicitly a register passed between stencils. Other shared data is passed in memory via stack offset.
        - Each stencil can have "pass-through" arguments, using widest data type.
            - So you have a stencil for equality "==" of ints with two registers, with stack argument and constant, with passthrough and any of the above, etc.
            - (Isn't this still an exponential explosion? Maybe # passthrough is configurable.)
        - Stencil library takes Stencil configuration -> Stencil binary (with patch records).
    - Code generation
        - Apparently this is extremely fast, maybe even faster than AST construction. They do post-order traversals, first to plan register usage, then to select stencil configurations and construct a compact CPS call graph.
        - Once we have the call graph, it does a topological ordering and pastes in the stencils while applying patches to the correct addresses.
        - "In our benchmarked implementations, we only use registers to preserve temporary values produced while evaluating an expression." — equivalent to a simple Sethi-Ullman register allocation algorithm, where they just spill over registers that exceed the watermark.
        - So actually constructing stencils falls back to Clang and requires some parsing of object files, but once the stencils are there, codegen is a simple scan / pattern match.
        - Decided mem2reg (hot local variables -> registers) is not worth it. Registers are only used for expression intermediates, not local variables.
    - Stencil generator (MetaVar) internals
        - Paper writes a bunch of C++ template metaprogramming, there's constants like numMaxPassthroughs and `OperandType` generic that control the number of variants.
        - Holes are represented as linker relocations in the object file.
        - LLVM Object File API is used to extract stencils, convert them to compact binary format.
    - Now reading PEP 744, it gives some historical background. Since Python 3.11, they've been collecting micro-ops and speculative interpretation with some type information on bytecode. But this is turned off right now due to overheads. Compilers can reduce those overheads in instruction decoding, memory traffic, etc.
    - __Despite their reputation, JIT compilers are not magic “go faster” machines.__
    - CPython already uses a C-based DSL to generate most of the interpreter (since 3.12), so maintainers don't have to handwrite boilerplate for each instruction. Then this DSL can also be used to update the copy-and-patch template JIT, reducing maintenance cost.
        - Current implementation has about 900-lines of build-time Python, and 500 lines of runtime C. That's a very small amount of code for a JIT system!
    - Interesting to see how this all works. Lots of discussion about not making Python itself more complex, while still gaining performance improvements and reusing infrastructure. Adds LLVM to the build dependencies though. Target "5% better" on at least one platform.
    - "Improving JIT code quality" ticket https://github.com/python/cpython/issues/115802
