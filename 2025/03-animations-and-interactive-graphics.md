- Animation and interactive graphics (Mar 2025)
    - Starting off strong by compiling a list of a bunch of websites, pretty neat.
    - Animated Vega-Lite (VIS '23)
        - Goal is to unify interaction and animation. Remember that these two are related: both of them slice up the data based on some parameter, either time or some other click.
        - Time as an __encoding__ versus as an __event stream__.
        - You can drag a slider, and that would be equivalent to the clock moving forward in an animation. Kind of like the playback timeline in a video. But interaction can be for things other than time in Vega, like focusing on a particular series.
        - Zoom/pan as a "view transformation."
        - Vega-Lite has two encodings "x" and "y" for spatial position, as well as other encodings like "hue" and "size" — this paper introduces one for "time" as a field of data. Infer a default for tweening between points based on class of data, so you get animations!
        - Interesting that the default is 500ms per domain value. Information density?
        - "rescale" by time: interesting property, is this parameter in the rest of Vega-Lite as well? Feels like it could be applied to other slices / interactions with data.
        - Specific examples motivate the design of the grammar. I think this makes sense as a guiding principle. Over time, when designing creative tools / languages, you can look at what people have created in the past, and then discover how to express that again.
        - Sidenote: Lol the paper refers to itself as "low viscosity" instead "frictionless"!
    - Lottie animated graphics format https://lottie.github.io/lottie-spec/latest/single-page/
        - Recall that the purpose is to be exported from Adobe After Effects as a vector graphic, then embedded into software via the runtime library.
        - Compact JSON format. Gradients are a flat array of numbers, color & opacity stops.
        - Some properties can be animated. An animated property consists of an array of __keyframes__. Each keyframe is a timestamp, as well as two cubic bezier handle points for the interval before and after that keyframe. They form a cubic curve from from [0,0] to [1,1].
        - Just skimming. Besides keyframes and animations (spec unclear to me right now), the rest of the format is just: SVG shapes, colors, stroke/fill/mask/line caps, and transforms. Simple!
        - Animation = name, layers[], framerate, framenumbers, width/height, assets (reference, data URIs), markers (named time ranges), and slots (constants).
            - Layers can have children, so they follow the movement, kind of like an SVG group. It forms a transform hierarchy. Also they can have an ip..op range of frames in which they're visible (optimization for sequencing?).
            - Types of layers: solid bg, image (by asset ID), precomposition (component with other layers that can be transformed or referenced again), null (group), or shapes.
        - Overall a pretty simple format. Basically like SVG, but with precomposition, enter/exit times, and every property (scalars, position, color, gradients) is allowed to be a list of keyframes that are played back in animation.
    - Post-processing shaders https://blog.maximeheckel.com/posts/post-processing-as-a-creative-medium/
        - Obviously dithering is possible, but I like this blog post's trick of rounding (u,v) in a shader as their efficient means for pixelating an image. Why downscale it yourself, when you can have the graphics pipeline do it properly for you with implcit mipmaps?
        - Lots of ideas with the pixelation, in the rough category of "shapes" or "colors" per pixel.
        - Seems fun to design around this for web graphics! They're __very easy__ postprocessing shaders to write, just a few lines of code, and very interactive.
        - Sidenote: The on-demand interactive code editors embedded into the website are pretty damn clean. They also rerender immediately. It's understated but very good frontend work to build this well.
        - Speaking of advanced image effects, you can do a lot with CSS! These image filters for simulating the light on cards are kind of awesome. https://poke-holo.simey.me/
    - More 3D work to collect from people
        - https://darkroom.engineering/
        - https://www.joshuas.world/
        - https://qrshow.nyc/
    - Reminded of 3D work
        - Google gravity, the original inspiration for a lot of this.
        - Case study, Lusion's connectors. https://codesandbox.io/p/sandbox/xy8c8z
            - The source of this is actually quite simple. No keyframes, just a physics engine with impulse -x applied on each frame, and a kinematic sphere collider.
            - Three different materials. Pull in the glTF for the connectors.
            - Some tech here.
                - N8AO = SSAO library https://github.com/N8python/n8ao
                - "Lightformer" = dynamic construction of HDRI environment lights with React, doesn't cost extra ray intersections like real lights do(?)
            - Overall only like ~100 lines of code, but you need to be comfortable knowing where all the controls are. Like setting near/far/fov in Blender, versus props on the scene.
                - Can use a design engineering workflow. Start with a main component, gradually add more stuff. But mix it in with creative execution, tweaking stuff.
            - Remember that the library editor is one way to construct these graphics, but it requires writing code. There's a bunch of merit to exploring in actual creative tools / software like After Effects, Blender, or Rive. They're purpose-built UIs instead of PL.
            - But I guess code helps capture custom logic in a way that isn't so easy. Tends toward anything more __procedural__, … up to generative art, or video games. Even postprocessing can be done better with node graph editors.
    - How does N8AO work (SSAO for three.js)
        - Two shaders: EffectShader and PoissonBlur, both WebGL.
        - There's some glue code to bind all the uniforms and inputs/outputs together. It seems to use textures and involves view / projection matrices, not sure exactly how though.
        - Ah okay, it takes the depth, normals, and albedo. Then calculates occlusion based on the depth buffer results.
        - TBN matrix = local coordinate system around the normal vector (tangent + bitangent). It uses blue noise jittering around each pixel to grab a few other samples, then weights them and reduces variance.
        - SAMPLES = 16, takes random samples along a hemisphere and then projects them back to world position and loads a point from the depth map.
        - Second part is a **Poisson blur**. Raw SSAO values are low sample count and noisy / jittered with random blue noise. So then it applies a Poisson disc blur (Poisson = random samples to avoid aliasing), weighted by a distance kernel.
    - March 30. Might spend this meeting thinking about maps.
        - Questions about tiles: What's the difference between actual terrain information and layers on top of that? How many layers are included in a tile server, and what's the standard? What free tile servers or datasets are available?
        - Question (practical): What's the best way to draw / customize a map on a website and to dynamically style it? How about post-processing the image or adding isolines? Surely it's not too hard of a systems problem.
        - Frontend: MapLibre GL vs Mapbox GL (both Mapbox API) vs Leaflet (less powerful).
            - Seems like Leaflet is probably slower and not optimized with WebGL, probably also not a very modern appearance.
            - MapLibre offers real-time data visualization, dynamic styling, GPU acceleration.
            - Interesting that [deck.gl](https://deck.gl/) exists as a library too. This looks like a pretty neat API for data visualization on maps. It's like the map itself acts as a canvas, and `react-map-gl` plus the deck.gl / loaders.gl ecosystem display data on top.
            - "styles.json" loaded from https://basemaps.cartocdn.com/gl/dark-matter-nolabels-gl-style/style.json — see https://carto.com/basemaps
            - CSV file in hex bin demo is just a list of lat,lng pairs. Then they're converted to [lat,lng] pairs by JavaScript before being passed in. `gpuAggregation` is interesting.
            - `transitions: { elevationScale: 3000 }` for the fade in animation.
            - `mapStyle` is set to a style.json URL, but where's the tile server URL when you're using the `<Map>` component from Maplibre? I'll check the Network tab to see what's going on here, maybe there's a hardcoded default.
                - Oh I just realized that the style file actually has the source as a URL in that file. `"sources": { "carto": { "type": "vector", "url": ".../tiles.json" } }`
                - https://tiles.basemaps.cartocdn.com/vector/carto.streets/v1/tiles.json
                - https://tiles-b.basemaps.cartocdn.com/vectortiles/carto.streets/v1/5/16/9.mvt
            - MVT = Mapbox Vector Tiles, a Protobuf of path winding number-based shapes.
        - Okay, getting more into the format here. The rendering library is code that takes ["style" documents](https://maplibre.org/maplibre-style-spec/) and turns them into interactive, rendered maps.
            - https://en.wikipedia.org/wiki/World_Geodetic_System
            - There are like at most 22-ish levels of zoom in maps (high resolution). This matches the Earth's diameter, which is about 12 million meters or 2^23.
            - Z=0 is the world in 1 tile. Z=14 is about 1.5 km per tile, or a neighborhood level.
            - At Z=22, you can see individual cars or benches. Each pixel is 2 cm.
            - "Web Mercator" (EPSG:3857) is coordinate system for point, linestring, or polygon. Each named layer in the tile has features, and each feature has key-value properties.
            - [tippecanoe](https://github.com/mapbox/tippecanoe) turns GeoJSON/Geobuf/CSV features into tiles.
                - > The goal of Tippecanoe is to enable making a scale-independent view of your data, so that at any level from the entire world to a single building, you can see the density and texture of the data rather than a simplification from dropping supposedly unimportant features or clustering or aggregating them.
            - The style files take a source, take a "source-layer" string within that source, and filter it by one or more conditions (`property == value`). Then you can add fill/background, which are colors graded by the zoom level.
            - You might also only show these layers at different minzoom / maxzoom levels.
    - Demos now!
        - People making small scenes in three.js!
        - mc-bench has this thing to render Minecraft scenes with commands in AI models.
        - https://design.google/library/exploring-color-google-maps
