- GPU kernel programming (Feb 2025)
    - CUDA programming cheatsheet (PMPP)
        - cudaMalloc, cudaMemcpy = device global memory
            - global & constant memory > block shared memory > thread registers & local memory
            - `__device__ float DeviceFunc` = device internal
            - `__global__ void KernelFunc` = host calls device
            - `__host__ float HostFunc` = ordinary function, you can use this along with device to get isomorphic libraries on both host + device
        - threadIdx.x, threadIdx.y = index of thread inside its block (Thread 17,26) — blockDim
        - blockIdx.x, blockIdx.y = index of block inside the grid (dispatch of kernel) — gridDim
        - `__syncthreads()` only synchronizes within a block, execution between different blocks is order-independent and cannot have data dependencies, CUDA can execute individual blocks in any order it chooses at runtime.
        - Warp = 32. Unit of thread scheduling in SMs, kind of like a cache line. Your blockDim should be a multiple of 32.
            - Avoid thread divergence within a warp. SIMT: every thread proceeds in lockstep.
            - Use favorable data access patterns. DRAM is not exactly random-access. Within a warp, access sequential data (row-major vs column-major = GMEM coalescing).
        - Sizing: You would have between 128-1024 threads per block, given block and thread per-SM limits. In practice, most people start with 128-256 threads per block.
            - blockDim should be on the order of a single SM. In T4-A10-A100-H100 GPUs, you get like maximum of 16 blocks per SM. But blockDim also can't be too big, since then it won't fit on a single SM and can't be scheduled at all.
            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fekzhang%2FYAugZxPZ_V.png?alt=media&token=263b5ba3-d40e-448b-a2dc-235b8d8cb6c7)
        - How to use shared memory? (64-228 KB)
            - Tiling for matrix multiplication: Every thread loads one element of the shared memory array. Then syncthreads(), do your computation on all, and syncthreads() one more time.
            - Also, prefetch tiles into registers during computation to do data load during independent compute. (Register size = shared memory size.)
    - Applications to scientific computing. Think in parallel, try to avoid data dependencies, in geometric applications you can schedule compute smarter.
    - Notes from siboehm — kernel optimization and tuning
        - The rough idea seems to be to start with a blueprint for this kernel (load stuff into SMEM, in the proper order with vectorization, then multiply). Then tune all the numeric parameters to suit a particular workload on hardware.
            - For example, global memory coalescing (2) is just, changing iteration order of output from [x][y] to [y][x]. This translates an incoherent load on A into a coherent load on B.
            - Then, smem cache-blocking (3) shares the accessed memory across 32 threads.
        - Arithmetic intensity (AI) computed as FLOPs / GMEM access.
            - But although smem cache-blocking increases AI by 16x (~1 -> 15), and thus fitting under the GMEM bandwidth bound, it's still not close to reaching this limit. (And CUDA already does pretty good L1 caching automatically.)
            - There's many other parts to this calculation. Not just improving arithmetic intensity.
        - Occupancy = ratio of active/resident warps per SM, to total number of warps per SM. I think this is ~equivalent to threads since warp = thread*32. For example, there a 64 resident warps per SM on the A100, so you'd want to saturate that.
            - In theory then, 2 blocks is fine right, for this kernel? But the blog post says it isn't fine. I don't totally understand this — I think it might just be an illustrative example, but as they mention, SMEM isn't the bottleneck in this specific case. It's registers & threads.
            - Register count and SMEM capacity are the main limits to occupancy. You basically can't exceed the total L1 cache available to that SM.
            - Q: How to count resource demands (registers / thread)? Profiling tools?
        - Reading the profiler output
            - See mix of executed instructions by count: LDS, FFMA, IADD3.
            - See percentage of cycles in each warp state: Documented in [Nsight guide](https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference).
            - So you see, if you have a lot of memory stalls in your kernel, and you also look at the PTX and see more latency in LD than FMA… it's time to start blocktiling.
            - This kind of optimization is just another way of increasing intensity. But the performance / profiling gives us higher confidence, and we can directly improve the kernel by reducing the memory stalling — less guesswork!
        - Basically, kernels 3->4 and 4->5 all improve increase the number of work done per thread by 8x, then consequently decreases the number of GMEM accesses per element by 2x. The reason why is tricky.
            - Kernel 3->4 uses BM=BN=64 and BK=8. This means each block computes a 64x64 tile of output, but only uses 2x8x64 = 1024 input floats in shared memory. This would usually be too many threads! But since each thread computes an 8x1 submatrix of outputs (warp = 8x32), block size = 512. (Basically we add an inner loop `dotIdx` to each thread).
            - Kernel 4->5 increases AI more. Each thread computes an 8x8 tile (TM=TN=8). Also, BM=BN=128, wow! And BK=8 as before. So we end up with block size of 16x16=256, but still less GMEM accesses.
            - At this point, SMEM access also can be optimized within block-tiling — this is where adding register caches helps. You can populate the relevant TN+TM entries into __registers__ before each iteration of BK. It's threadtiling!
        - Kernel 9 is autotuning of BM, BN, BK, TM, and TN. Straightforward.
        - Warptiling is yet another level that we can add on top of thread / blocktiling. I don't completely understand, but it seems to be just another loop based on having each __warp__ compute a given number of output elements in its dimension, same idea.
        - To summarize: Every hardware level divides up the compute a bit, with its own caching and scheduling primitives. The caches and vectorization mean that we need to change our iteration order to suit the hardware. Tile compute smartly to increase occupancy, reduce GMEM / SMEM accesses. Along the way, use profilers & tune parameters.
    - JAX: Pallas kernel language notes
        - `import jax.experimental.pallas as pl`
        - This is an embedded language in Python based on the concept of a "Ref". A bit higher-level, kind of like Triton in not needing to worry about the details of warptiling / blocktiling / threadtiling. Tries to maintain optimal control.
        - All the kernels so far use `[...]` to get the whole block.
        - You might want to write kernels differently on various devices. SMEM/GMEM loading. TPUs are like very wide SIMD registers, while GPUs are warp schedulers.
        - "Ref" = mutable buffers in SMEM. JAX has you chunk up the kernel into blocks.
        - `pl.BlockSpec` is the magic sauce that does the tiling and caching. Basically tells you how to carve up accessing each input for each output. In matmul, this is pretty simple, but you could implement blocktiling on it. Declarative instead of for-loops.
        - Fusing a tanh activation function onto a kernel is trivial.
        - Interesting: on TPUs, each operation has a cost. Addition, subtraction, multiplication are fast. But division/exp/tanh/pow are slower, and sin/cos are the slowest.
    - Tinygrad codebase notes
        - Small codebase, tries to be "everything" in a tiny package ~10K lines (it's dense).
        - Getting quite good Metal performance with Tinygrad, even when `BEAM=0` is disabled and just manual optimizations are turned on.
        - Interesting part is probably the backend, `get_kernel(Render, UOp) -> Kernel`. Applies optimizations in order and then does a Renderer pass. Kernel.opts is the "Renderer" that has all accelerator and dialect-specific stuff, like CUDA/Metal/C/PTX and so on.
            - required_optimizations() — seems minor, skip for now, one UPCAST.
            - apply_tensor_cores() — if device has TC, run optimizations that transform operations into tensor core arithmetic (__how?__)
            - hand_coded_optimizations() — if not TC, then there's about 100 lines of optimization passes, very short (__how does this work, and why is it so simple?__)
            - beam_search() — If beam search enabled (not by default), run beam search. This is slow but the results are cached after first evaluation. Runs the kernel many times.
        - On my M1 Pro, matmul is like 20x faster after hand_coded_optimizations() and another 2x faster after that with apply_tensor_cores(). So it's clear that the optimization magic is happening somewhere in both of these two functions.
            - (Also, the fact that it's device-independent is very impressive!)
        - UOp is the tree of executed operations, kind of like a trace. It's constructed lazily when you chain ops, and then `Tensor.realize()` actually executes the whole thing.
        - OptOps is TC, UPCAST, UNROLL, LOCAL, GROUP, GROUPTOP, NOLOCALS, PADTO, SWAP.
            - They can also be applied to a specific axis number or operation argument.
            - About the kernel itself: each tensor is multidimensional, and they have a coloring system to determine which parts of the kernel ("inner loops") correspond to each axis.
            - ```python
                # there's eight chunks of the shape
                # blue   -- global dims
                # cyan   -- local dims (warp ones first)
                #  *** self.first_reduce
                # green  -- reduce-local dims
                # red    -- reduce loops
                #  *** self.upcasted
                # purple -- reduce upcasted
                # yellow -- normal upcasted dimensions
              ```
        - I'm guessing that each of the Opt classes does some recoloring of the axes in each operation, and then the colors are used to generalized an optimized kernel in codegen.
        - Okay, on further inspection of the tc and handrolled opts (exclude beam search):
            - Handrolled: Matvec multiply (upcast reduce), group and fuse reduction axes, image data types (stride=4), heuristics to also upcast non-reduce axes, and then local grouping.
            - **UPCAST** is mentioned in many places, what does this do? (yellow)
            - The other important ones are GROUP[TOP], LOCAL, UNROLL.
            - TC/PADTO are only used in tensor core opts, that's a separate thing. I can take a look at that later. Using a special instruction for matrix multiplication.
            - Note: Almost all operations can be represented in terms of the same shape algebra, this is why the code is so short!
        - At the last step of compiling a program, Renderer.render() then takes the linearized list of UOp (previously a tree) and lowers it into machine code for the backend.
            - A couple backends, most inherit from CStyleLanguage (Clang, OpenCL, Intel, Metal, CUDA, AMD, WGSL).
            - Some are special cases, like LLVM IR and PTX.
            - In all cases the code is __very short and dependency-free__. Just format strings.
        - Mysterious "swizzle" parameter in TensorCore. Looks like an odd permutation. It might have to do with how you arrange the first 10 axis numbers. Empirically determined?
    - More notes on Tinygrad (with Chenyu presenting!)
        - Biggest file is tensor.py, which basically all the high-level APIs on a tensor (matches Torch / JAX). Wrapper over an array of memory or something.
        - Different runtimes, `python -m tinygrad.device` to show supported devices.
            - Quick notes: inside `runtime/`, ops_gpu is OpenCL, disk is something to do with random-access memory, hip is AMD HIP, amd is AMD RDNA 3, Intel is OpenCL + XMX tensor cores, ops_nv is Nvidia without CUDA (PTX?), qcom is Qualcomm
            - DTypes supports ints, floats (no float8 yet), images.
        - tinygrad.nn: optimizers, mnist/cifar datasets, and model state (safetensor, tar, ggml)
        - tinygrad.gradient: has a `PatternMatcher` for each of the ~20 ops that maps them to the gradient of the op, very compact list of ops, not even a specialized convolution!
            - Philosophy of tinygrad is to simplify things down to as few ops as possible, and these are encoded in UOp. "If you get new hardware, you're not going to write a conv gate on it."
        - Tensor -> lazydata -> ScheduleItem -> ExecItem -> codegen -> source code -> compiled binary -> runtime
        - tinygrad.engine: everything before codegen, there's lots of stuff here!
            - schedule.py: tensor graph, **grouping (kernel fusion)** and linearizer for topological sorting, as well as the memory planning
            - realize.py: ScheduleItem -> ExecItem (1 per kernel), can be pickled and sent to nodes
            - search.py: BEAM search and MCTS
            - jit.py: replay execution with new data? similar to ExecItem? — it can be used to speed up a repeated computation, can store a whole CUDA graph = multiple kernels
            - multi.py: NCCL and ring allreduce system
            - memory.py: "very straightforward" memory planning system
        - tinygrad.codegen: kernels, optimization actions
            - Kernel.to_program() -> ProgramSpec
            - Sticking to searches on real devices for now, rather than a "smart" simulator or something else. This helps you avoid problems.
            - linearize.py: order into blocks, any topological ordering of blocks is a valid program
            - lowerer.py: rewrites or permutes buffer indexes into tensor / array indexes
            - rewriter.py: general library for simplification rules on UOp trees; const folding, folding load/store, etc.; and every compile / lowering step has its own rewrite rules
            - transcendental.py: implement sin, exp2, log2, pow if device doesn't support it natively
            - __(currently doesn't support some common tricks like double-buffering, local memory copies)__
        - tinygrad.shape: representing multi-dimensional arrays
            - view.py: lazy zero-copy views / permute on arrays
                - __trivia: determining the index to access in the buffer after a reshape() requires int division/modulo, which is slow on gpus! this is why modern gpus have dedicated integer units__
            - shapetracker.py: shape-tracking algebra, stack of multiple views, tracking properties throughout the views (maybe locality is better? use OptOps.SWAP)
                - __Operation to merge two views / ShapeTracker.simplify() can be applied. But the algebra is surprising, you can have 3 views that are not piecewise merge-able but all 3 can be merged. Apparently it's NP-hard lol.__
        - tinygrad.renderer: take code blocks from codegen, make them into real source code
            - Fun fact: for WGSL, browsers limit the size of your buffer, so you need to chunk the weights when running Llama.
            - Can learn and try some GPU programming with WebGPU!
        - tinygrad.runtime: copy memory between host/device, allocate RAM, & dispatch kernels
            - Some backends have RDMA.
            - These all just call into different ways to allocate a buffer, run a program ~100 LoC.
            - ops_cloud.py: send GPU backend operations over USB, HTTP (1 req per ExecItem)
            - ops_amd.py: they believe in AMD hardware
            - support/hcq.py: [Hardware Command Queue](https://docs.tinygrad.org/developer/hcq/)
            - graph/: Maybe you run llama, and it's like 300 kernels (ExecItem), or you train a big network and each step is 50,000 kernels — limited by queue dispatch time (each ~ns). This is CUDA Graph, you replay the queue of multiple kernels / save the dispatch for later.
        - tinygrad.viz: when `VIZ=1`, display a webapp of all the UOp graphs / rewrites
            - Convention for global buffers: input = 1 and output = 0
        - Environment variables
            - DEBUG=1..7 — show kernel dispatch, codegen, timing and shapes, gets more specific
            - TRACEMETA=3 — show line number of the code
            - BEAM=2..4
            - NOOPT=1
        - What is swizzle? It's apparently in the GPU docs, you need to swizzle your input into a specific register order, before calling into a tensor core instruction.
        - Currently looking for gains on better memory planning & kernel fusion.
        - OptOps
            - GROUP = […][…][…][…], an i++ loop for each thread
            - GROUPTOP = strided groups, like the "k" axis in blocktiling
            - UPCAST = float4 type, very fast
            - UNROLL = loop unrolling (similar to UPCAST in principle)
            - LOCAL = ???
