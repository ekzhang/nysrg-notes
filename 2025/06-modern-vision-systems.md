- Modern Vision Systems (Jun 2025)
    - Recap on generative models / [diffusion theory from scratch](https://iclr-blogposts.github.io/2024/blog/diffusion-theory-from-scratch/) — there are two dual ways of thinking about the DDPM work, one in terms of modeling score via Langevin dynamics / statistical mechanics, and one in terms of reversing a Gaussian diffusion process.
        - To estimate the score scalably, previously people used all kinds of complex objective functions. Key insight that made it work in DDPM was diffusing the sample data into Gaussians, and then only modeling the score function on the __parts that matter__ (i.e., the paths from the sample data to eventually reach their destination), not the whole space.
        - Last bit of the equation is the __schedule__ (beta): this basically tracks how much of the information you lose as you diffuse to Gaussians on each step / add noise (forward). It then affects the reverse equation / denoising process.
        - People use cosine schedules because they look good and empirically get better detail, but any schedule is theoretically grounded.
        - DDPM uses Fokker-Planck equation and Anderson's reversal to explain the math, versus the Langevin dynamics explanation (reverse-first) that builds up the theory chronologically.
        - Note that the score function depends on time t, this is an input to the learned model (U-Net for Stable Diffusion) via a sinusoidal timestamp embedding.
    - Practical diffusion models / advancements.
        - Some more steps are needed to go from the common math to actually usable results. This is why we had an explosion of papers after DDPM.
        - As for the actual neural network architecture:
            - Residual block: x -> conv1 -> (+time embedding) -> activation -> conv2 -> (+residual). 
            - Attention layers: self-attention in middle block, and cross-attention with text or image conditioning in encoder and decoder stages.
            - Encoder blocks use 2x strided convolutions, decoder blocks use 2x upscaling.
            - `attention_resolutions` (say: 4, 8, 16) — at how much downsampling do you introduce attention blocks. These come between residual blocks, and they include both multi-head self-attention (like transformer encoder) and cross-attention. OK, easy enough.
        - Convolution is used by SD 1.5 / SDXL, other models like [Imagen](https://papers.nips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf) vary the U-Net architecture or make it more efficient.
        - Also [this paper](https://arxiv.org/pdf/2102.09672) "Improved DDPMs" (2021) looks like it was a pretty big practical advancement in efficiency of these models. It uses cosine schedule in forward pass and __learns__ the reverse variances via interpolation trick, model decides how confident it is in adjusting the variances.
        - Another [big paper](https://arxiv.org/abs/2010.02502) DDIMs (2020) that this built on, an efficiency improvement to see the code for. It's just a different "sampler" — and various models will either predict epsilon, x_0, or v (velocity). The sampler is kind of just a numerical integrator for the reverse SDE, basically how you actually go from scores to samples via the reverse Ornstein-Uhlenbeck (damped Langevin) process.
        - Most popular sampler today seem to be DPM++ or variants. Basically, all samplers converge to the same under infinite time steps, kind of like how RK4 and Euler do the same thing. But practical models today use 4-30 steps, and then the algorithm choice matters!
        - See [How Imagen Actually Works](https://www.assemblyai.com/blog/how-imagen-actually-works) for another blog post take on this. There's some more steps as you piece everything together: the U-Net diagram, STM/MTL super-resolution models, and classifier-free guidance (interpolating between guided=1 and unguided=0).
        - This is why the models are __systems__, they take the foundational math and create complex things out of it as they design model architectures.
        - DPM++ solvers are from [this paper](https://arxiv.org/pdf/2211.01095), including DPM++2M.
    - Alright, back to image backbones. Will read a bit about these.
        - EfficientViT calls itself an "image foundation model." Is this a compelling description of what it does, or is it more of a backbone? I guess it depends on how it can be used.
        - See this [recent comparison of Vision Transformers](https://arxiv.org/pdf/2308.09372) from February 2025. It breaks down the improvements into three categories: 1) faster attention or hybrid convolutional-attention algorithms / O(n^2) self-attention bottleneck, 2) removing / merging some of the token sequence, and 3) changing up the final MLP block.
    - Apple's Core ML
        - Apple's Core ML is surprisingly open-source and has a lot of details. What is the `.mlmodel` format, and how does inference actually work? Sounds like it's a lot faster in iOS 15+.
        - Their examples are older vision models (ResNet50, MobileNetV3-Small), sounds like this hasn't been updated too recently.
        - Utilities for quantizing model weights, other compression methods. The flow is pretty standard, you build out your model in PyTorch/TensorFlow or other frameworks, then trace and export it into a format compatible with Apple.
        - I guess that [OpenELM](https://apple.github.io/coremltools/docs-guides/source/convert-openelm.html) (2024) is a newer model from Apple, for Core ML deployment.
        - Unclear how the actual inference engine works, on-device. It's definitely a familiar mobile deployment story, and the company is in charge of the whole runtime engine. But I think, maybe the open-source ecosystem should be responsible for this.
    - Grounding DINO
        - Object detection model (DETR = detection transformer), built on [GLIP](https://github.com/microsoft/GLIP), which extends CLIP to object-level descriptions within an image. That's why it's grounded.
        - Previous object detection models might only let you detect predefined classes, but that doesn't generalize well. Rather than asking the model to do that, you can use text-image embeddings and then ask for captioning / detecting novel classes of objects.
        - Often plugged into robotics pipelines now.
        - Some people are using it for masked image generation with Stable Diffusion. Results seem somewhat questionable, but I get it.
    - SAM 2
        - Meta model for image segmentation from human descriptions.
        - What's the model architecture?
    - Vision model foundations
        - Let's drill into ViT models and then alternatives that explore their computational efficiency and rationale behind the model architecture.
        - MobileVit (2021) https://arxiv.org/abs/2110.02178
        - EfficientFormer (2022 / 2023) https://github.com/snap-research/EfficientFormer
        - Question: What happened after 2023? Do people not study this direction anymore? (It sounds like they're still the best models out there, but maybe progress stalled. Or the ChatGPT launch led people to focus on other kinds of research.)
        - [CLIP-UP](https://machinelearning.apple.com/research/clip-up) (2025) — A Simple and Efficient Mixture-of-Experts CLIP Training Recipe with Sparse Upcycling
        - [MetaFormer](https://arxiv.org/pdf/2111.11418) (2022)
            - This is a criticism / analysis of the core idea that makes vision transformers work. Also mentions / compares to architectures like ViT, ConvNeXt. Got here by starting on the FastViT paper and learning about the architecture.
            - It's not necessarily the self-attention heads. What matters is a "token mixer" module, having something that combines information between spatial tokens.
            - The idea of a transformer is, image -> patch embedding -> blocks, where each block is [norm, <mix>, residual, norm, MLP-ratio4, residual] like a transformer.
            - Except the <mix> doesn't need to be self-attention, it could even just be a high-pass filter with 0 parameters. Or as in the FNet paper, they replace attention with a Fourier transform that also does not have any parameters!
            - PoolFormer gets reasonable results, despite lacking self-attention. Patch stride 4, then /8, /16 and /32.
                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fekzhang%2FmX9vnQT6vC.png?alt=media&token=dc77e7a1-87ec-4f2b-b262-dfaf0e954fe9)
            - An interesting relationship here with ConvNeXt, which is a very popular transformer that has __depthwise convolution__ blocks and 1x1 convolutions. The "pool" operation is like a fixed-weight depthwise convolution, and the 1x1 convs are like per-token MLP blocks.
            - So, ConvNeXt has an almost identical block design compared to MetaFormer, I guess this is where the key idea comes from originally.
                - MetaFormer just takes it to the logical conclusion by eliminating attention.
            - Something to think about with all of these models — what's the __training recipe__? I often don't think about this too much since I'm just doing inference, but it's important to think about where the datasets are coming from, and what we can reproduce practically. Otherwise, it's just a bit out of reach for people outside of the big labs.
        - FastViT (2023) https://github.com/apple/ml-fastvit
            - Newer paper than EfficientFormer, ConvNeXt, Swin-T, CMT.
            - Interesting that they also still compare to EfficientNet-B4, and ResNet-50. Guess those traditional convolutional neural networks are still interesting to people.
            - __"novel token mixing operator, RepMixer, a building block of FastViT, that uses structural reparameterization to lower the memory access cost by removing skip-connections in the network"__
            - Nice, it's the top model on the [Core ML Models page](https://developer.apple.com/machine-learning/models/) on Apple right now. The smaller variant T8 is only 6.5 MB, headless, and runs in less than a millisecond!
            - They also replace all convolutions with depthwise + pointwise counterparts. These convolutions are typically used in stem/patch embedding layers.
                - Note: patch embedding = ViT, stem embedding = ConvNeXt and FastViT.
                - Patch embeddings are rigid divisions of the image, while stem embeddings are just a bit more overlapped, perhaps stronger local features.
            - RepMixer(X) = DepthwiseConv(BN(X)) + X. At inference time, with batch size 1, this can just be reparameterized to a single DepthwiseConv!
            - Lol this is funny, they compared it to PoolFormer and found a 43% latency reduction due to the lack of skip connections! Less memory accesses.
                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fekzhang%2FG2glEvhPzy.png?alt=media&token=0ddb7d9c-1114-4c42-9e58-235242a6aa14)
            - For semantic segmentation, they use the [Semantic FPN](https://arxiv.org/abs/1901.02446) decoder (2019).
            - Project ideas: take your webcam, run some segmentation on it, apply cool filters like reflective light effects (Vercel-ish) or physics.
    - Rectified flow
        - I guess I'm returning to some of the diffusion model stuff, since it's the most fun and mathy part. Also I like geometry, so seeing these ODE / SDE models is cool!
        - History of models:
            - SD 1.1-1.4 = original by CompVis with the same model architecture and different training checkpoints (LAION), then SD 1.5 = Runway, SDXL / SD 3.x = Stability AI.
            - After Stability AI collapsed, the researchers moved on to Black Forest Labs. They are releasing FLUX.1 with different variants and sizes. Each of these models is pretty flexible (conditioning, output size, …), and Schnell is the smallest. It's what powers the image generation on the Modal.com site, which is very fast.
            - Looks like state-of-the-art has moved on from diffusion (DDPM / DDIM / …) into rectified flow transformers (SD 3.5, FLUX.1). Biggest models are expensive and not public, but they're also not __that__ much better than the open-source available ones.
        - https://rectifiedflow.github.io/ — rectified flow tutorial, series of good posts
        - Going to take some notes from reading / skimming the [flow book](https://www.cs.utexas.edu/~lqiang/PDF/flow_book.pdf) though. This one is about 100 pages long, and I think the complete exposition would help.
        - Ch 1. Rectified Flow
            - You write an ODE that takes $$Z_0 \sim \pi_0$$ and moves it along $$\dot Z_t = v_t(Z_t), t \in [0, 1]$$.
            - Note that the velocity curve is time-dependent. Can solve this using Euler's method or other solvers. Many flows but aim to have __straight transports__ for computational efficiency, since you accumulate less error in each step.
            - The "rectified flow" idea is to take an arbitrary coupling of $$\pi_0$$ (noise) and $$\pi_1$$ (model distribution), then just train the ODE model $$v_t$$ by interpolated paths from each pair in the coupling. __Hand-wavy part:__ The parts where the paths "cross over" naturally average out during training, and we're left with a rectified model.
            - Rectify() can be used on any time-differential stochastic process. It produces a more optimal transport, since there's less cost / Wasserstein metric? Also during inference you get a "straighter" path.
            - You can do it with any interpolation process, $$(\alpha_t, \beta_t)$$ where alpha: 0->1 and beta: 1->0. This encodes time scaling. When alpha+beta=1, you get linear interpolation, and alpha^2+beta^2=1 gives you DDPM/DDIM (spherical).
            - I think I can see how the loss term matches up between flow matching and DDPM-style score matching! In either case — you train by averaging over random couplings of data to noise, and random timesteps, taking L2 error. So they're kind of the same quantity, just interpreted differently though score and flow.
            - Things to vary: loss function, time-weighting by $$\eta_t$$. You can also improve the sampler by adding a bit of steady-state Langevin dynamics on the distribution of $$Z_t$$, this provides some negative feedback but can reduce detail if too much noise is added. You need the score $$\nabla \log \rho_t(Z_t)$$ to do this, but that's obtainable in closed form from the flow thanks to [Karras et al., 2022](https://arxiv.org/pdf/2206.00364).
            - Historically it developed as diffusion, then diffusion as ODEs (DDIM), then just learning flow directly — with possibly some diffusion in the sampler.
        - Ch 2. Marginals and Errors
            - There's some math here to prove that Rectify() preserves marginals for all t. If you have a time-differential stochastic process $$\{X_t\}$$, and $$\{Z_t\} = \mathrm{Rectify}(\{X_t\})$$, then we want $$X_t \sim Z_t$$ to match for all t.
            - Okay they did a couple manipulations with derivatives + Adam's law which amounted to basically, ending up at some integral equations of a test function over the distributions $$\pi_t$$, based on $$v_t^X(X_t) = \mathbb E[\dot X_t \mid X_t]$$ the normalizing flow. Then they cited a random theorem from analysis to finish it off, a uniqueness result for flow solutions via measure theory.
            - Another intuition to think of it: it's kind of like a continuity equation. If you have two flows in different directions, you can kind of average them out, and the same "quantity" of probability mass flows in aggregate.
            - Next, they study the KL divergence, Bergman, and semantic loss of these marginal distributions with ODE vs SDE. I didn't look at this too closely.
        - Ch 3. Interpolations and Equivalence
            - If you pass the $$X_t$$ process through a time-variant diffeomorphism $$\phi_t(x)$$, and/or a time schedule $$\tau_t$$, this commutes with rectification. (ok seems reasonable, the averaging is a local property)
            - In particular, this means all the different affine transformations are equivalent, with different alphas, betas, and time schedules. (…really? wat)
            - They also prove that straight and spherical are the same, up to a reparameterization. Their explanation is the identity $$\dot \alpha_t' \beta_t' - \alpha_t' \dot\beta_t' = \pi/2 = \text{constant}$$. (Note that the book has a typo.) Seems believable enough, cool — this means spherical vs linear doesn't matter in training if true.
            - Euler method can be viewed as piecewise linear approximation of the flow. But if you're doing a "curved interpolation scheme", you want __natural Euler samplers__ that is invariant/equivariant. Includes DDIM, DDPM, EDM, DMP solvers.
            - Natural Euler = instead of line segment, take the same interpolated curve at each point, but extended tangent to the velocity. This is same as Euler for linear interpolation, but for spherical (affine), you use some trig identities and get the same formula as DDIM. So DDIM is derived as a special case of natural Euler.
            - [Their blog post has a great visual of this, applied to DDIM.](https://rectifiedflow.github.io/blog/2024/DDIM-and-natural-euler/)
            - It's mentioned that you can derandomize __stochastic__ smooth interpolations by just Rao-Blackwellizing away the randomness, lol. But this is definitely a case where the theory doesn't quite hold in practice.
        - Ch 4. Identities
            - This is where they derive Tweedie's formula and score functions for straight interpolation to a normal distribution. Nice! This is what lets us add additional noise with Langevin dynamics during sampling, for flexibility. ("Karras", I think)
            - Some more identities with covariance, curvature, L2 error bounds. Fine.
        - Ch 5. Stochastic Solvers
            - Alright, here is where they get back to the main course. ODE can have a stochastic solver by re-injecting a bit of Langevin dynamics, tuned as needed. ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fekzhang%2FlktyvtxGKv.png?alt=media&token=98fa32ab-a066-4fde-877b-857f366bda37)
            - Tweedie's formula in Ch 4 is how we compute the score in the second term.
            - Why bother with Langevin dynamics? It's a __self-correcting mechanism__ or something, reduces outliers. Maybe a little counterintuitive, but __larger diffusion yields more concentrated samples__. Why?
            - We recap Euler–Maruyama / Brownian motion and Fokker–Planck Equation, which drifts particles away from high density (i.e., by negative score function).
            - "It is known that Langevin dynamics can be viewed as the gradient flow of the KL divergence … under the 2-Wasserstein metric … a gradient flow in the space of distributions" wtf
                - My attempt to translate to English: given any distribution, if you apply LD to it, you get closer to the target distribution by KL divergence. In fact, you get __maximally closer__ per unit of Wasserstein metric ("earth moved").
                - So the intuition is that LD corrects distributional drift. Great!
            - Something about different noise schedules here, including derivations for the original DDPM paper.
        - Ch 6. Reward Tilting
            - Here "tilting" means that we weight the final distribution, in a Bayesian sense where we multiply it by a reward function $$r(x)$$ and then renormalize. I think an example is a censor that weights improper images with $$r(x)=0$$.
                - My first instinct is to just do importance sampling. But this is slow / not guaranteed. You'd much rather get a new flow, so you don't waste time.
                - "The question is how … preferably without retraining the model."
            - So they tilt it, and now we're trying to recover properties of the tilted process by doing some mathematical tricks.
            - Oh I see. They end up with "Gaussian tilting" results that tilt the model in terms of L2 distance away from a target point $$x^*$$, with linear/spherical schedules. Basically updating the velocity field to point more in that direction, but mathematically sound.
            - Algorithm also can be used with negative factor to steer away from a point.
            - Seems of questionable utility given existing text-conditioning.
        - Reflections
            - The math is nice, although they spend a lot of time just thinking about this abstract idea of "rectification" in theory, which may or may not actually align with what's going on with rectification in practice.
            - The "neural network as a learned function" is doing __a lot__ of heavy lifting in this description. The dynamics of training the neural network probably are equally as important as the flow itself, since it's due to the inductive biases / continuity that any rectification happens at all, with floating-point numbers being inexact.
            - Also the 2D example they have of the o=>o flow with crossing paths on a square is iconic. It might be the fundamental picture of what's going on.
            - They have a bunch of algebraic formulas. It must take quite a bit of practice to convert between the different time schedules, diffusion ODE/SDE terminology, and interpolations in your head. At least this book is relatively consistent in its notation, unlike papers.
    - What is actually in SD3?
        - [Scaling Rectified Flow Transformers for High-Resolution Image Synthesis](https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf)
        - This paper starts with rectified flows ("not yet decisively established as standard practice") and then tries 61 different combinations of loss, schedule, on a few datasets.
        - Also parameterize based on model depth, run a bajillion scaling experiments.
        - The model architecture itself is a pretty straightforward extension of [Diffusion Transformers (DiT)](https://arxiv.org/pdf/2212.09748), where the latent image is split into patches. Except with two modalities for images and text, which are sent in parallel with different weights (but combined for self-attention).
          ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fekzhang%2F2N8XN3KkBU.png?alt=media&token=1fde0c20-7e1e-4c1b-80b8-b3f7567abc0a)
        - I guess one interesting thing is how they use CLIP-G/14, CLIP-L/14, and T5 XXL together — the former two are used in the adaptive layer norm weights (adaLN), which isn't like the cross-attention in prior U-Nets. But the latter is principally used as text tokens & passed through the DiT blocks directly. I wonder why this worked for them.
        - Alright, this seems pretty straightforward. One thing to remember here is that "straight line" refers to the forward / generative process for rectified flows, as opposed to DDPM. The sampling process is still always going to involve multiple ODE/SDE iterations.
    - Stepping through FLUX.1 Schnell code
        - Going to take some notes on this process here. Getting started, I downloaded the two big weights files, ae.safetensors and flux1-schnell.safetensors. These have some metadata and then a few hundred big array buffers in bfloat16 format.
        - First issue is loading them using the safetensors library. Unfortunately, this doesn't support bfloat16 for JAX. It only has that for torch. So that's an ecosystem annoyance.
        - When I run `t = f.get_tensor("decoder.conv_in.bias")` on the flax mode, it gives me float32 dtype. The shape is right at least, and the weights look reasonable.
        - I guess I can load them in numpy mode, then run jnp.array(…, dtype=jnp.bfloat16).
        - BTW, I think I won't have enough RAM on my laptop for this whole thing. Will start by developing the autoencoder though, and then maybe we can move to a Modal notebook (or function) on H100 for the rest of the implementation.
        - Flow might end up being, rapidly push changes to a Git repo, then `pip install` from that Git repo inside my notebook? Kind of jank TBH.
        - [Penzai](https://penzai.readthedocs.io/en/stable/notebooks/how_to_think_in_penzai.html) is cool with its pretty-printing and visualization tools for research, too tricky to get used to right now though, I just want to get something running.
        - Ok hm, I got a bit further and think it's probably just going to be more productive for me to first copy/paste big chunks of code into a notebook and explore it by changing things up or profiling it, rather than reimplementing everything from scratch at first.
        - I'm midway through, got the VAE latents, also figured out the difference FLUX.1 dev/schnell and other models are actually just the same architecture, 12B parameters each.
            - Difference is training process, schnell takes fewer diffusion steps, maybe trained with a bias toward straight-line ODE flows?
        - Rope = rotational position embeddings, they come in pairs. Applied proportionally to each channel, so channel i and position j has rot(i / theta^(dim/max_dim)) embedding. This makes sense — frequency is exponential with dimension number.
        - Alright, almost there. Copying was definitely the move, can skip over parts I'm not so interested in __at the moment__ like pretrained encoder submodules. Now I'm running into an issue where the model weights that I loaded are all Float32 instead of Bfloat16, which seems like a mistake when loading the safetensors files.
            - Ah, I'm dumb, I just didn't write `.to(torch.bfloat16)` during initialization.
            - Ok it works now. Glad I have LSP support and AI in this notebook environment, the delay isn't so bad honestly and it's very helpful when coding at 1 AM.
        - Need to restart kernel, oops. My messing around has used up all the GPU memory, and I think IPython is keeping around references to the variables that I printed out in cells, which prevents reclaiming the model memory. D:
        - There goes probably another 5-10 minutes, will come back. Back down to 33 GB / 80 GB.
        - Great, we did it! I understand a bit better where the surfaces for creative control here are. A lot of the workflows fit well into torch primitives, with some loose wrappers. It strikes me as a good balance between flexibility and structure — most modules in this codebase do what you expect, but they can also be modified from the outside, by autocast or no_grad or device casting and so on. Not too much model edits needed.
        - https://gist.github.com/ekzhang/663b6bccd1bfa1848c02a45afa171cef
