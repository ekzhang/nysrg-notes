- Ceph — [[May 19th, 2024]]
    - “Clients typically interact with a metadata server (MDS) to perform metadata operations (open, rename), while communicating directly with OSDs to perform file I/O (reads and writes), significantly improving overall scalability.”
    - “Our target workload may include such extreme cases as tens or hundreds of thousands of hosts concurrently reading from or writing to the same file or creating files in the same directory. Such scenarios, common in scientific applications running on supercomputing clusters, are increasingly indicative of tomorrow’s general purpose workloads.”
    - Why is separating data from metadata more scalable? It's not really justified in the paper. Ori questions, what if we just stored the metadata in the same RADOS object store?
    - __Dynamic Subtree Partitioning__ is an interesting idea, I had thought about doing something similar for Blobnet at [[Modal]] for instance. I am surprised that all of these ideas are almost 20 years old and used in one of the most popular distributed file systems in the world.
    - Sec 3. Some of the capabilities seem fundamentally at odds with each other: cache revocations when multiple clients open a file, for instance. It sounds like Ceph is trying to do __too much__ while also being POSIX-compliant, and that adds complexity. Being a "tool for everyone" is hard.
        - “Most notably, these include an __O_LAZY__ flag for open that allows applications to explicitly relax the usual coherency requirements for a shared-write file.”
        - Slightly relaxed consistency through silently caching the readdirplus operation.
        - However, stopping multiple writers whenever anyone `stat()` calls the file is kind of a dealbreaker for performance. I guess that's the price you pay, unfortunately.
        - lazyio_propagate and lazyio_synchronize
        - I mean, yeah, POSIX was designed for disks that are attached to the machine. It's different when you're running a distributed storage service! The requirements vary greatly.
    - Sec 4. “Although the MDS cluster aims to satisfy most requests from its in-memory cache, metadata updates must be committed to disk for safety. A set of large, bounded, lazily flushed journals allows each MDS to quickly stream its updated metadata to the OSD cluster in an efficient and distributed manner.”
        - If an MDS fails, other nodes can scan from the journal to recover, which is in RADOS.
        - Files with multiple hard-links are a special case for the "anchor table" —lol.
        - Dynamic subtree partitioning offers hierarchical locality, while still automatically splitting metadata between different MDS servers.
        - Selectively replicate popular directories and load balance between them.
        - Details are omitted about the actual metadata partitioning. It's a completely different paper (Ori: "secret fourth paper in the trilogy") from the author's PhD. Very complicated.
    - Sec 5. CRUSH and RADOS are pretty succinctly described. Failure recovery and metadata updates are the main complications here.
        - Storing objects on individual nodes was done with EBOFS, then Btrfs and XFS, and now they have eventually moved to Bluestore (direct disk drivers) and RocksDB.
    - “Finally, Ceph’s metadata management architecture addresses one of the most vexing problems in highly scalable storage—how to efficiently provide a single uniform directory hierarchy obeying POSIX semantics with performance that scales with the number of metadata servers.”
    - Ori: Mentions that if he did it again today, would probably use something like S3? Just because it's cheaper. And FoundationDB, perhaps, since that is well-tested.
    - Cool anecdotes about many companies still using fast on-prem storage, network-attached storage (NAS), NFS, etc., so all of this is still relevant today!
